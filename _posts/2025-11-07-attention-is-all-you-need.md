---
layout: post
title: "Understanding 'Attention Is All You Need'"
date: 2025-11-07
categories: [NLP, Transformers]
---

**Paper:** [Vaswani et al., 2017 â€” Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

### ğŸ§© Why I Chose This Paper
This paper introduced the **Transformer architecture**, which became the foundation for almost every modern language model (like BERT, GPT, and T5).  
I wanted to understand how it replaced RNNs and LSTMs with a more efficient mechanism.

---

### ğŸ’¡ Problem It Solves
Before Transformers, sequence models like RNNs and LSTMs struggled with **long-term dependencies** and **slow training**.  
This paper proposed using **self-attention** to capture relationships between all words in a sequence simultaneously â€” no recurrence needed.

---

### ğŸ” Core Idea
The key concept is **self-attention**, where each word looks at every other word in a sentence to decide whatâ€™s important.  

For example:
> In â€œThe cat sat on the mat,â€  
> the word *â€œcatâ€* attends to *â€œsatâ€* and *â€œmatâ€* to understand the sentence meaning.

The model uses:
- **Multi-head attention** â†’ multiple perspectives at once  
- **Positional encoding** â†’ keeps word order info  
- **Encoder-decoder structure** â†’ processes and generates sequences

---

### ğŸ“Š Results
Transformers achieved **state-of-the-art performance** on translation tasks and trained **much faster** than RNNs.

---

### âœï¸ My Takeaways
- Self-attention is both simple and powerful.  
- Removing recurrence made it easier to scale models massively.  
- Reading this paper helped me see why Transformers dominate NLP today.

---

### ğŸ”— Further Reading
- *â€œThe Illustrated Transformerâ€* by Jay Alammar  
- *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)*  
